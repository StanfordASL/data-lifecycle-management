{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import scod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST, KMNIST, EMNIST\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scripts.train_functions import create_dataloaders, main, set_up_model, load_ckp\n",
    "from scripts.benchmark_functions import alg_flags, scod_flagger, ds_scod_flagger, create_scod_model, create_benchmark_seq_batches, get_Lt_J\n",
    "from scripts.utils import eval_scod, init_scod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"~/data/\"\n",
    "batch_size = 100\n",
    "dataset_name = \"mnist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mnist_train': <torch.utils.data.dataloader.DataLoader object at 0x7fb0ab513e90>, 'mnist_test': <torch.utils.data.dataloader.DataLoader object at 0x7fb0ab513cd0>, 'fashion': <torch.utils.data.dataloader.DataLoader object at 0x7fb0ab513fd0>, 'kmnist': <torch.utils.data.dataloader.DataLoader object at 0x7fb07433d050>, 'emnist': <torch.utils.data.dataloader.DataLoader object at 0x7fb07433d1d0>}\n"
     ]
    }
   ],
   "source": [
    "dataloaders, dataset_sizes = create_dataloaders(root, batch_size, dataset_name=dataset_name)\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add SCOD wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight space dimension: 4.443e+04\n",
      "computing basis\n",
      "using T = 304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc89ea24b98416bb4ec5a15779f621d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_model_path = \"./best_model/mnist/best_model.pt\"\n",
    "\n",
    "unc_model = create_scod_model(load_model_path, dataset_name, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mixture dataset using flagged inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test seq  0  algorithm  0  flagged:  [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  1  algorithm  0  flagged:  [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  2  algorithm  0  flagged:  [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  3  algorithm  0  flagged:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  4  algorithm  0  flagged:  [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  5  algorithm  0  flagged:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  6  algorithm  0  flagged:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  7  algorithm  0  flagged:  [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  8  algorithm  0  flagged:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  9  algorithm  0  flagged:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_3  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_4  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_5  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_6  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_7  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_8  to list of dataloaders.\n",
      "Added  mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_9  to list of dataloaders.\n"
     ]
    }
   ],
   "source": [
    "ood_ratio = 0.99\n",
    "mix_train_ratio = 0.01\n",
    "num_batches = 10\n",
    "\n",
    "ood_num = int(batch_size*ood_ratio)\n",
    "mix_train_num = int(batch_size*mix_train_ratio)\n",
    "\n",
    "batch_compositions = [  {'mnist_test':batch_size - ood_num,'fashion':ood_num}]*num_batches\n",
    "train_batch_compositions = [{'mnist_train':mix_train_num}]*num_batches\n",
    "\n",
    "flag_limit = ood_num\n",
    "# algs_to_test = [lambda x: scod_flagger(x, unc_model, flag_limit=flag_limit, debug = False, dist_layer = scod.distributions.CategoricalLogitLayer()), \n",
    "#                 lambda x: ds_scod_flagger(x, unc_model, flag_limit=flag_limit, dist_layer = scod.distributions.CategoricalLogitLayer())]\n",
    "algs_to_test = [lambda x: scod_flagger(x, unc_model, flag_limit=flag_limit, debug = False, dist_layer = scod.distributions.CategoricalLogitLayer())]\n",
    "algs_names = ['scod_flagged']\n",
    "\n",
    "flagged_inputs, flagged_labels, flagged_fnames, flags = alg_flags(algs_to_test, dataset_name, batch_size, num_batches, batch_compositions)\n",
    "train_inputs, train_labels, train_fnames = create_benchmark_seq_batches(dataset_name, mix_train_num, num_batches, train_batch_compositions)\n",
    "for k in range(len(algs_to_test)):\n",
    "    for i in range(num_batches):\n",
    "        flagged_subset = [(ts, lbl) for (ts,lbl,flg) in zip(flagged_inputs[i], flagged_labels[i], flags[i][k]) if flg]\n",
    "        train_subset = [(ts, lbl) for (ts,lbl) in zip(train_inputs[i], train_labels[i])]\n",
    "        refine_subset = flagged_subset + train_subset\n",
    "        random.shuffle(refine_subset)\n",
    "\n",
    "        mix_name = \"mixture_\"+str(batch_compositions[i])+'_'+algs_names[k]+'_'+str(len(flagged_subset))+'_train_'+str(len(train_subset))+\"_batch_\"+str(i)\n",
    "        \n",
    "        dataloaders[mix_name] = DataLoader(refine_subset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        dataset_sizes[mix_name]  = len(refine_subset)\n",
    "        print(\"Added \", mix_name, \" to list of dataloaders.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_epoch_idx =  17\n",
      "valid_loss = 0.042841\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.7131\n",
      "val mnist_test Loss: 0.0428\n",
      "val fashion Loss: 5.9405\n",
      "val kmnist Loss: 8.5930\n",
      "Epoch 19/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.6765\n",
      "val mnist_test Loss: 0.0427\n",
      "val fashion Loss: 5.8659\n",
      "val kmnist Loss: 8.5580\n",
      "Epoch 20/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.6083\n",
      "val mnist_test Loss: 0.0426\n",
      "val fashion Loss: 5.7625\n",
      "val kmnist Loss: 8.5091\n",
      "Epoch 21/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.5136\n",
      "val mnist_test Loss: 0.0425\n",
      "val fashion Loss: 5.6360\n",
      "val kmnist Loss: 8.4483\n",
      "Epoch 22/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.3973\n",
      "val mnist_test Loss: 0.0425\n",
      "val fashion Loss: 5.4923\n",
      "val kmnist Loss: 8.3779\n",
      "Epoch 23/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.2656\n",
      "val mnist_test Loss: 0.0426\n",
      "val fashion Loss: 5.3372\n",
      "val kmnist Loss: 8.3004\n",
      "Epoch 24/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 5.1247\n",
      "val mnist_test Loss: 0.0427\n",
      "val fashion Loss: 5.1756\n",
      "val kmnist Loss: 8.2177\n",
      "Epoch 25/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.9770\n",
      "val mnist_test Loss: 0.0427\n",
      "val fashion Loss: 5.1588\n",
      "val kmnist Loss: 8.2090\n",
      "Epoch 26/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.9616\n",
      "val mnist_test Loss: 0.0428\n",
      "val fashion Loss: 5.1412\n",
      "val kmnist Loss: 8.1999\n",
      "Epoch 27/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.9454\n",
      "val mnist_test Loss: 0.0428\n",
      "val fashion Loss: 5.1228\n",
      "val kmnist Loss: 8.1903\n",
      "Epoch 28/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.9286\n",
      "val mnist_test Loss: 0.0428\n",
      "val fashion Loss: 5.1038\n",
      "val kmnist Loss: 8.1805\n",
      "Epoch 29/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.9111\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0843\n",
      "val kmnist Loss: 8.1703\n",
      "Epoch 30/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.8931\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0643\n",
      "val kmnist Loss: 8.1599\n",
      "Epoch 31/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.8748\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0440\n",
      "val kmnist Loss: 8.1492\n",
      "Epoch 32/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_0 Loss: 4.8561\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0420\n",
      "val kmnist Loss: 8.1482\n",
      "Epoch 18/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2660\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0398\n",
      "val kmnist Loss: 8.1470\n",
      "Epoch 19/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2637\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0376\n",
      "val kmnist Loss: 8.1459\n",
      "Epoch 20/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2612\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0352\n",
      "val kmnist Loss: 8.1446\n",
      "Epoch 21/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2586\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0328\n",
      "val kmnist Loss: 8.1434\n",
      "Epoch 22/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2560\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0304\n",
      "val kmnist Loss: 8.1421\n",
      "Epoch 23/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2532\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0279\n",
      "val kmnist Loss: 8.1408\n",
      "Epoch 24/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2504\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0276\n",
      "val kmnist Loss: 8.1407\n",
      "Epoch 25/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2501\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0273\n",
      "val kmnist Loss: 8.1405\n",
      "Epoch 26/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2498\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0271\n",
      "val kmnist Loss: 8.1404\n",
      "Epoch 27/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2495\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0268\n",
      "val kmnist Loss: 8.1403\n",
      "Epoch 28/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2492\n",
      "val mnist_test Loss: 0.0429\n",
      "val fashion Loss: 5.0265\n",
      "val kmnist Loss: 8.1401\n",
      "Epoch 29/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2489\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0262\n",
      "val kmnist Loss: 8.1400\n",
      "Epoch 30/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2486\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0260\n",
      "val kmnist Loss: 8.1398\n",
      "Epoch 31/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2483\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0259\n",
      "val kmnist Loss: 8.1398\n",
      "Epoch 32/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_1 Loss: 5.2483\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0259\n",
      "val kmnist Loss: 8.1398\n",
      "Epoch 18/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3897\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0259\n",
      "val kmnist Loss: 8.1398\n",
      "Epoch 19/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3896\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0259\n",
      "val kmnist Loss: 8.1398\n",
      "Epoch 20/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3896\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0258\n",
      "val kmnist Loss: 8.1398\n",
      "Epoch 21/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3896\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0258\n",
      "val kmnist Loss: 8.1397\n",
      "Epoch 22/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3895\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0258\n",
      "val kmnist Loss: 8.1397\n",
      "Epoch 23/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3895\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0258\n",
      "val kmnist Loss: 8.1397\n",
      "Epoch 24/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3895\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0258\n",
      "val kmnist Loss: 8.1397\n",
      "Epoch 25/32\n",
      "----------\n",
      "train mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_2 Loss: 5.3895\n",
      "val mnist_test Loss: 0.0430\n",
      "val fashion Loss: 5.0258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-afffd7c155db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# batch loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mfd_id\u001b[0;34m(fd)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# this doesn't work with shared memory handles, which is why we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# support the \"file_descriptor\" sharing method on that platform.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_ino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Resume training on train set\n",
    "# phases = [('train','mnist_train'), ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "\n",
    "# Resume training on mixture\n",
    "# phases = [('train',\"mixture_{'mnist_test': 90, 'fashion': 10}_scod_flagged_10_train_100\"), \n",
    "#             ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "# phases = [('train',\"mixture_{'mnist_test': 90, 'fashion': 10}_scod_flagged_10_train_1\"), \n",
    "#             ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "# phases = [('train',\"mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1\"), \n",
    "#             ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "\n",
    "for b in range(num_batches):\n",
    "    initialized_model, exp_lr_scheduler, sgd_optimizer, criterion, device = set_up_model(dataset_name)\n",
    "    load_model_path = \"./best_model/mnist/best_model.pt\"\n",
    "    model, optimizer, start_epoch_idx, valid_loss = load_ckp(load_model_path, initialized_model, sgd_optimizer)\n",
    "    print(\"start_epoch_idx = \", start_epoch_idx)\n",
    "    print(\"valid_loss = {:.6f}\".format(valid_loss))\n",
    "\n",
    "    start_epochs = start_epoch_idx\n",
    "    num_epochs = 15\n",
    "    scheduler = exp_lr_scheduler\n",
    "\n",
    "    phases = [('train',\"mixture_{'mnist_test': 1, 'fashion': 99}_scod_flagged_99_train_1_batch_\"+str(b)), \n",
    "            ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "\n",
    "    losses = {p[1]: np.zeros(num_epochs) for p in phases}\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(start_epochs, start_epochs+num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, start_epochs+num_epochs))\n",
    "        print('-' * 10)\n",
    "        for phase in phases:\n",
    "            if phase[0] == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # batch loop\n",
    "            for d in dataloaders[phase[1]]:\n",
    "                if len(d) == 2:\n",
    "                    inputs, labels = d \n",
    "                elif len(d) == 3:\n",
    "                    inputs, labels, fnames = d \n",
    "                else:\n",
    "                    raise ValueError(\"Unrecognized. Dataloader entry has length {}\".format(len(d)))\n",
    "                labels = labels.flatten().type(torch.LongTensor)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase[0] == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase[0] == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase[1]]\n",
    "            print('{} {} Loss: {:.4f}'.format(phase[0], phase[1], epoch_loss))\n",
    "            if phase[0] == 'train':\n",
    "                scheduler.step()\n",
    "            losses[phase[1]][epoch-start_epochs] = epoch_loss\n",
    "\n",
    "    # Save train losses and valid losses\n",
    "    losses_path = \"./losses/\"\n",
    "    fname = \"batch_\"+str(b)+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +'.npz' \n",
    "    with open(losses_path+fname, \"wb\") as fp:\n",
    "        pickle.dump(losses, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train losses and valid losses\n",
    "losses_path = \"./losses/\"\n",
    "fname = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +'.npz' \n",
    "# with open(losses_path+fname, \"wb\") as fp:\n",
    "#     pickle.dump(losses, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training in a new way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_epoch_idx =  17\n",
      "valid_loss = 0.042841\n"
     ]
    }
   ],
   "source": [
    "initialized_model, exp_lr_scheduler, sgd_optimizer, criterion, device = set_up_model(dataset_name)\n",
    "load_model_path = \"./best_model/mnist/best_model.pt\"\n",
    "model, optimizer, start_epoch_idx, valid_loss = load_ckp(load_model_path, initialized_model, sgd_optimizer)\n",
    "print(\"start_epoch_idx = \", start_epoch_idx)\n",
    "print(\"valid_loss = {:.6f}\".format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/18\n",
      "----------\n",
      "Weight space dimension: 4.443e+04\n",
      "computing basis\n",
      "using T = 304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0529bc437f54782b711dce7bb35f3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty:  2.294168472290039\n",
      "LtJ shape is  (10, 44426)\n",
      "New Uncertainty:  2.294233798980713\n",
      "Trace is  0.0057763914\n",
      "Uncertainty:  2.2781007289886475\n",
      "LtJ shape is  (10, 44426)\n",
      "New Uncertainty:  2.2781505584716797\n",
      "Trace is  0.039836433\n",
      "Uncertainty:  2.2669413089752197\n",
      "LtJ shape is  (10, 44426)\n",
      "New Uncertainty:  2.266752243041992\n",
      "Trace is  -0.010459036\n",
      "Uncertainty:  2.258082151412964\n",
      "LtJ shape is  (10, 44426)\n",
      "New Uncertainty:  2.2576828002929688\n",
      "Trace is  0.052654635\n",
      "train mixture_{'mnist_test': 90, 'fashion': 10}_scod_flagged_10_train_100 Loss: 0.3520\n",
      "val mnist_test Loss: 0.0427\n",
      "val fashion Loss: 5.8917\n",
      "val kmnist Loss: 8.5776\n",
      "{\"mixture_{'mnist_test': 90, 'fashion': 10}_scod_flagged_10_train_100\": array([0.35199595]), 'mnist_test': array([0.04272059]), 'fashion': array([5.89165637]), 'kmnist': array([8.57758451])}\n"
     ]
    }
   ],
   "source": [
    "start_epochs = start_epoch_idx\n",
    "num_epochs = 1\n",
    "scheduler = exp_lr_scheduler\n",
    "\n",
    "# Resume training on train set\n",
    "# phases = [('train','mnist_train'), ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "\n",
    "# Resume training on mixture\n",
    "phases = [('train',\"mixture_{'mnist_test': 90, 'fashion': 10}_scod_flagged_10_train_100\"), \n",
    "            ('val','mnist_test'), ('val','fashion'), ('val','kmnist')]\n",
    "\n",
    "losses = {p[1]: np.zeros(num_epochs) for p in phases}\n",
    "\n",
    "dist_layer = scod.distributions.CategoricalLogitLayer()\n",
    "\n",
    "# Resume training\n",
    "for epoch in range(start_epochs, start_epochs+num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1, start_epochs+num_epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Create new unc model\n",
    "    new_unc_model = init_scod(model, dataloaders[\"mnist_train\"], dataset_name) \n",
    "\n",
    "    for phase in phases:\n",
    "        if phase[0] == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # batch loop\n",
    "        for d in dataloaders[phase[1]]:\n",
    "            if len(d) == 2:\n",
    "                inputs, labels = d \n",
    "            elif len(d) == 3:\n",
    "                inputs, labels, fnames = d \n",
    "            else:\n",
    "                raise ValueError(\"Unrecognized. Dataloader entry has length {}\".format(len(d)))\n",
    "            labels = labels.flatten().type(torch.LongTensor)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase[0] == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                \n",
    "\n",
    "                if phase[0] == 'train':\n",
    "                    for i in inputs[:2]:\n",
    "                        unc = eval_scod(i, unc_model, dist_layer)\n",
    "                        unc = unc.item()\n",
    "                        print(\"Uncertainty: \", unc)\n",
    "                        LtJ = get_Lt_J(i.unsqueeze(0), unc_model, debug=False, dist_layer=dist_layer)\n",
    "                        orig_trace = np.trace(LtJ)\n",
    "                        print(\"LtJ shape is \", np.shape(LtJ))\n",
    "\n",
    "                        new_unc = eval_scod(i, new_unc_model, dist_layer)\n",
    "                        new_unc = new_unc.item()\n",
    "                        print(\"New Uncertainty: \", new_unc)\n",
    "                        new_LtJ = get_Lt_J(i.unsqueeze(0), new_unc_model, debug=False, dist_layer=dist_layer)\n",
    "                        diff = np.trace(new_LtJ - LtJ)\n",
    "                        print(\"Trace is \", diff/orig_trace)\n",
    "\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_sizes[phase[1]]\n",
    "        print('{} {} Loss: {:.4f}'.format(phase[0], phase[1], epoch_loss))\n",
    "        if phase[0] == 'train':\n",
    "            scheduler.step()\n",
    "        losses[phase[1]][epoch-start_epochs] = epoch_loss\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mixture dataset using flagged inputs and weighting by uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test seq  0  algorithm  0  flagged:  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, False, True, True, True]\n",
      "Uncertainty:  2.293386936187744\n",
      "Uncertainty:  2.296449899673462\n",
      "Uncertainty:  2.2916970252990723\n",
      "Uncertainty:  2.295175552368164\n",
      "Uncertainty:  2.2911081314086914\n",
      "Uncertainty:  2.2927355766296387\n",
      "Uncertainty:  2.294149398803711\n",
      "Uncertainty:  2.2983007431030273\n",
      "Uncertainty:  2.2959296703338623\n",
      "Uncertainty:  2.299753189086914\n"
     ]
    }
   ],
   "source": [
    "ood_ratio = 0.1\n",
    "mix_train_ratio = 1.0\n",
    "num_batches = 1\n",
    "\n",
    "ood_num = int(batch_size*ood_ratio)\n",
    "mix_train_num = int(batch_size*mix_train_ratio)\n",
    "\n",
    "batch_compositions = [  {'mnist_test':batch_size - ood_num,'fashion':ood_num}]\n",
    "train_batch_compositions = [{'mnist_train':mix_train_num}]\n",
    "\n",
    "flag_limit = ood_num\n",
    "# algs_to_test = [lambda x: scod_flagger(x, unc_model, flag_limit=flag_limit, debug = False, dist_layer = scod.distributions.CategoricalLogitLayer()), \n",
    "#                 lambda x: ds_scod_flagger(x, unc_model, flag_limit=flag_limit, dist_layer = scod.distributions.CategoricalLogitLayer())]\n",
    "algs_to_test = [lambda x: scod_flagger(x, unc_model, flag_limit=flag_limit, debug = False, dist_layer = scod.distributions.CategoricalLogitLayer())] \n",
    "algs_names = ['scod_flagged']\n",
    "\n",
    "flagged_inputs, flagged_labels, flagged_fnames, flags = alg_flags(algs_to_test, dataset_name, batch_size, num_batches, batch_compositions)\n",
    "train_inputs, train_labels, train_fnames = create_benchmark_seq_batches(dataset_name, mix_train_num, num_batches, train_batch_compositions)\n",
    "for k in range(len(algs_to_test)):\n",
    "    for i in range(num_batches):\n",
    "        flagged_subset = [(ts, lbl) for (ts,lbl,flg) in zip(flagged_inputs[i], flagged_labels[i], flags[i][k]) if flg]\n",
    "        train_subset = [(ts, lbl) for (ts,lbl) in zip(train_inputs[i], train_labels[i])]\n",
    "        \n",
    "        if unc_model is not None:\n",
    "            dist_layer = scod.distributions.CategoricalLogitLayer()\n",
    "            for fs in flagged_subset:\n",
    "                unc = eval_scod(fs[0] , unc_model, dist_layer)\n",
    "                unc = unc.item()\n",
    "                print(\"Uncertainty: \", unc)\n",
    "        \n",
    "        # refine_subset = flagged_subset + train_subset\n",
    "        # random.shuffle(refine_subset)\n",
    "\n",
    "        # mix_name = \"mixture_weighted\"+str(batch_compositions[i])+'_'+algs_names[k]+'_'+str(len(flagged_subset))+'_train_'+str(len(train_subset))\n",
    "        \n",
    "        # dataloaders[mix_name] = DataLoader(refine_subset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        # dataset_sizes[mix_name]  = len(refine_subset)\n",
    "        # print(\"Added \", mix_name, \" to list of dataloaders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
