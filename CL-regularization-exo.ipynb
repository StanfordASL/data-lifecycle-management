{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import scod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scripts.train_functions import create_dataloaders, main, set_up_model, load_ckp\n",
    "from scripts.benchmark_functions import alg_flags, scod_flagger, ds_scod_flagger, create_scod_model, create_benchmark_seq_batches, get_Lt_J\n",
    "from scripts.utils import eval_scod, init_scod\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take a mixed batch (x% earth, y% lens flare, z% test)\\\n",
    "Flag OOD f% using scod k-highest/ds-scod\n",
    "\n",
    "2. Continual training: \\\n",
    "    a. Baseline method: mix new with old training\\\n",
    "    b. Our method: use EWC to prevent forgetting\\\n",
    "    Train for N epochs on mixture\n",
    "\n",
    "3. Plot loss evolution on \\\n",
    "    a. Mixture\\\n",
    "    b. Just validation set (in distribution)\\\n",
    "    c. Earth background (OOD 1)\\\n",
    "    d. Lens flare (OOD 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"datasets/exoromper/\"\n",
    "\n",
    "batch_size = 100\n",
    "dataset_name = \"exoromper\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_train': <torch.utils.data.dataloader.DataLoader object at 0x7f4d39684990>, 'all_val': <torch.utils.data.dataloader.DataLoader object at 0x7f4d07fc70d0>, 'all_test': <torch.utils.data.dataloader.DataLoader object at 0x7f4d39684a10>, 'space': <torch.utils.data.dataloader.DataLoader object at 0x7f4d39684dd0>, 'earth': <torch.utils.data.dataloader.DataLoader object at 0x7f4d39713950>, 'lens_flare': <torch.utils.data.dataloader.DataLoader object at 0x7f4d39713210>}\n"
     ]
    }
   ],
   "source": [
    "dataloaders, dataset_sizes = create_dataloaders(dataset_path, batch_size, dataset_name=dataset_name)\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add SCOD wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "force_cpu is True. device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somrita/anaconda3/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/home/somrita/anaconda3/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight space dimension: 2.563e+03\n",
      "computing basis\n",
      "using T = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b0ab115d75482397ad4170eca1375a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_model_path = \"./best_model/ex_v5_best_model.pt\"\n",
    "\n",
    "unc_model = create_scod_model(load_model_path, dataset_name, batch_size, force_cpu=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mixture dataset using flagged inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test seq  0  algorithm  0  flagged:  [True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Test seq  1  algorithm  0  flagged:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Added  mixture_{'space': 1, 'earth': 99}_scod_flagged_99_train_1_batch_0  to list of dataloaders.\n",
      "Added  mixture_{'space': 1, 'earth': 99}_scod_flagged_99_train_1_batch_1  to list of dataloaders.\n"
     ]
    }
   ],
   "source": [
    "ood_ratio = 0.99\n",
    "mix_train_ratio = 0.01\n",
    "num_batches = 2\n",
    "\n",
    "ood_num = int(batch_size*ood_ratio)\n",
    "mix_train_num = int(batch_size*mix_train_ratio)\n",
    "\n",
    "batch_compositions = [  {'space':batch_size - ood_num,'earth':ood_num}]*num_batches\n",
    "train_batch_compositions = [{'space':mix_train_num}]*num_batches\n",
    "\n",
    "flag_limit = ood_num\n",
    "algs_to_test = [lambda x: scod_flagger(x, unc_model, flag_limit=flag_limit, debug = False, dist_layer = scod.distributions.NormalMeanParamLayer(), force_cpu=True)]\n",
    "algs_names = ['scod_flagged']\n",
    "\n",
    "flagged_inputs, flagged_labels, flagged_fnames, flags = alg_flags(algs_to_test, dataset_path, batch_size, num_batches, batch_compositions)\n",
    "train_inputs, train_labels, train_fnames = create_benchmark_seq_batches(dataset_path, mix_train_num, num_batches, train_batch_compositions)\n",
    "for k in range(len(algs_to_test)):\n",
    "    for i in range(num_batches):\n",
    "        flagged_subset = [(ts, lbl) for (ts,lbl,flg) in zip(flagged_inputs[i], flagged_labels[i], flags[i][k]) if flg]\n",
    "        train_subset = [(ts, lbl) for (ts,lbl) in zip(train_inputs[i], train_labels[i])]\n",
    "        refine_subset = flagged_subset + train_subset\n",
    "        random.shuffle(refine_subset)\n",
    "\n",
    "        mix_name = \"mixture_\"+str(batch_compositions[i])+'_'+algs_names[k]+'_'+str(len(flagged_subset))+'_train_'+str(len(train_subset))+\"_batch_\"+str(i)\n",
    "        \n",
    "        dataloaders[mix_name] = DataLoader(refine_subset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        dataset_sizes[mix_name]  = len(refine_subset)\n",
    "        print(\"Added \", mix_name, \" to list of dataloaders.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "force_cpu is True. device:  cpu\n",
      "start_epoch_idx =  34\n",
      "valid_loss = 2.141914\n",
      "Epoch 35/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.5493\n",
      "val space Loss: 3.5746\n",
      "val earth Loss: 65.3320\n",
      "val lens_flare Loss: 29.5189\n",
      "Epoch 36/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.5989\n",
      "val space Loss: 5.5236\n",
      "val earth Loss: 65.5361\n",
      "val lens_flare Loss: 32.5425\n",
      "Epoch 37/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.4752\n",
      "val space Loss: 6.6172\n",
      "val earth Loss: 64.5890\n",
      "val lens_flare Loss: 33.4607\n",
      "Epoch 38/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.7625\n",
      "val space Loss: 7.1665\n",
      "val earth Loss: 61.7315\n",
      "val lens_flare Loss: 32.9585\n",
      "Epoch 39/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.5508\n",
      "val space Loss: 8.3264\n",
      "val earth Loss: 62.5490\n",
      "val lens_flare Loss: 34.6818\n",
      "Epoch 40/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.6353\n",
      "val space Loss: 8.6613\n",
      "val earth Loss: 62.0178\n",
      "val lens_flare Loss: 34.9250\n",
      "Epoch 41/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.6841\n",
      "val space Loss: 13.6188\n",
      "val earth Loss: 63.5326\n",
      "val lens_flare Loss: 40.4770\n",
      "Epoch 42/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.8407\n",
      "val space Loss: 10.9889\n",
      "val earth Loss: 60.5057\n",
      "val lens_flare Loss: 36.3878\n",
      "Epoch 43/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 5.0353\n",
      "val space Loss: 13.1035\n",
      "val earth Loss: 63.6250\n",
      "val lens_flare Loss: 40.4164\n",
      "Epoch 44/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 6.3961\n",
      "val space Loss: 22.5846\n",
      "val earth Loss: 65.7438\n",
      "val lens_flare Loss: 50.9519\n",
      "Epoch 45/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 5.0171\n",
      "val space Loss: 23.2667\n",
      "val earth Loss: 63.6329\n",
      "val lens_flare Loss: 50.1747\n",
      "Epoch 46/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.6607\n",
      "val space Loss: 22.5216\n",
      "val earth Loss: 65.0547\n",
      "val lens_flare Loss: 50.2775\n",
      "Epoch 47/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.9010\n",
      "val space Loss: 19.5761\n",
      "val earth Loss: 60.4192\n",
      "val lens_flare Loss: 45.0790\n",
      "Epoch 48/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.6601\n",
      "val space Loss: 20.4000\n",
      "val earth Loss: 63.5213\n",
      "val lens_flare Loss: 47.5221\n",
      "Epoch 49/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0 Loss: 4.7603\n",
      "val space Loss: 17.2824\n",
      "val earth Loss: 61.5253\n",
      "val lens_flare Loss: 43.5833\n",
      "force_cpu is True. device:  cpu\n",
      "start_epoch_idx =  34\n",
      "valid_loss = 2.141914\n",
      "Epoch 35/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.9729\n",
      "val space Loss: 2.5673\n",
      "val earth Loss: 64.7791\n",
      "val lens_flare Loss: 27.6964\n",
      "Epoch 36/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 5.1604\n",
      "val space Loss: 3.2525\n",
      "val earth Loss: 64.5680\n",
      "val lens_flare Loss: 29.3607\n",
      "Epoch 37/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 5.4330\n",
      "val space Loss: 7.7495\n",
      "val earth Loss: 65.3283\n",
      "val lens_flare Loss: 36.0111\n",
      "Epoch 38/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8497\n",
      "val space Loss: 7.3511\n",
      "val earth Loss: 64.4181\n",
      "val lens_flare Loss: 35.1359\n",
      "Epoch 39/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 5.0759\n",
      "val space Loss: 12.3076\n",
      "val earth Loss: 65.1287\n",
      "val lens_flare Loss: 40.9856\n",
      "Epoch 40/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8789\n",
      "val space Loss: 9.7823\n",
      "val earth Loss: 61.9802\n",
      "val lens_flare Loss: 36.8999\n",
      "Epoch 41/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8182\n",
      "val space Loss: 7.6944\n",
      "val earth Loss: 58.6561\n",
      "val lens_flare Loss: 33.1699\n",
      "Epoch 42/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8024\n",
      "val space Loss: 7.6493\n",
      "val earth Loss: 59.1208\n",
      "val lens_flare Loss: 33.4294\n",
      "Epoch 43/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8403\n",
      "val space Loss: 7.0925\n",
      "val earth Loss: 58.8640\n",
      "val lens_flare Loss: 32.6502\n",
      "Epoch 44/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.7393\n",
      "val space Loss: 7.3659\n",
      "val earth Loss: 56.7987\n",
      "val lens_flare Loss: 31.8747\n",
      "Epoch 45/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8186\n",
      "val space Loss: 6.2268\n",
      "val earth Loss: 55.8003\n",
      "val lens_flare Loss: 30.0898\n",
      "Epoch 46/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 5.2560\n",
      "val space Loss: 6.0874\n",
      "val earth Loss: 53.7402\n",
      "val lens_flare Loss: 28.9537\n",
      "Epoch 47/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.9966\n",
      "val space Loss: 6.3998\n",
      "val earth Loss: 55.1553\n",
      "val lens_flare Loss: 29.9468\n",
      "Epoch 48/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.9464\n",
      "val space Loss: 6.3318\n",
      "val earth Loss: 55.8477\n",
      "val lens_flare Loss: 30.1855\n",
      "Epoch 49/49\n",
      "----------\n",
      "train mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_1 Loss: 4.8548\n",
      "val space Loss: 6.3435\n",
      "val earth Loss: 56.8030\n",
      "val lens_flare Loss: 30.7299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for b in range(num_batches):\n",
    "    initialized_model, exp_lr_scheduler, sgd_optimizer, criterion, device = set_up_model(dataset_name, force_cpu=True)\n",
    "    load_model_path = \"./best_model/ex_v5_best_model.pt\"\n",
    "    model, optimizer, start_epoch_idx, valid_loss = load_ckp(load_model_path, initialized_model, sgd_optimizer)\n",
    "    print(\"start_epoch_idx = \", start_epoch_idx)\n",
    "    print(\"valid_loss = {:.6f}\".format(valid_loss))\n",
    "\n",
    "    start_epochs = start_epoch_idx\n",
    "    num_epochs = 15\n",
    "    scheduler = exp_lr_scheduler\n",
    "\n",
    "    phases = [('train',\"mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_\"+str(b)), \n",
    "            ('val','space'), ('val','earth'), ('val','lens_flare')]\n",
    "\n",
    "    # phases = [('train',\"mixture_{'space': 1, 'earth': 99}_scod_flagged_99_train_1_batch_\"+str(b)), \n",
    "    #         ('val','space'), ('val','earth'), ('val','lens_flare')]\n",
    "            \n",
    "\n",
    "    losses = {p[1]: np.zeros(num_epochs) for p in phases}\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(start_epochs, start_epochs+num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, start_epochs+num_epochs))\n",
    "        print('-' * 10)\n",
    "        for phase in phases:\n",
    "            if phase[0] == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # batch loop\n",
    "            for d in dataloaders[phase[1]]:\n",
    "                if len(d) == 2:\n",
    "                    inputs, labels = d \n",
    "                elif len(d) == 3:\n",
    "                    inputs, labels, fnames = d \n",
    "                else:\n",
    "                    raise ValueError(\"Unrecognized. Dataloader entry has length {}\".format(len(d)))\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase[0] == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase[0] == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase[1]]\n",
    "            print('{} {} Loss: {:.4f}'.format(phase[0], phase[1], epoch_loss))\n",
    "            if phase[0] == 'train':\n",
    "                scheduler.step()\n",
    "            losses[phase[1]][epoch-start_epochs] = epoch_loss\n",
    "\n",
    "    # Save train losses and valid losses\n",
    "    losses_path = \"./losses/\"\n",
    "    fname = \"ours_batch_\"+str(b)+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +'.npz' \n",
    "    with open(losses_path+fname, \"wb\") as fp:\n",
    "        pickle.dump(losses, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training in a new way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized_model, exp_lr_scheduler, sgd_optimizer, criterion, device = set_up_model(dataset_name)\n",
    "load_model_path = \"./best_model/ex_v5_best_model.pt\"\n",
    "model, optimizer, start_epoch_idx, valid_loss = load_ckp(load_model_path, initialized_model, sgd_optimizer)\n",
    "print(\"start_epoch_idx = \", start_epoch_idx)\n",
    "print(\"valid_loss = {:.6f}\".format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epochs = start_epoch_idx\n",
    "num_epochs = 1\n",
    "scheduler = exp_lr_scheduler\n",
    "\n",
    "# Resume training on mixture\n",
    "phases = [('train',\"mixture_{'space': 90, 'earth': 10}_scod_flagged_10_train_100_batch_0\"), \n",
    "            ('val','space'), ('val','earth'), ('val','lens_flare')]\n",
    "\n",
    "losses = {p[1]: np.zeros(num_epochs) for p in phases}\n",
    "\n",
    "dist_layer = scod.distributions.NormalMeanParamLayer()\n",
    "\n",
    "# Resume training\n",
    "for epoch in range(start_epochs, start_epochs+num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1, start_epochs+num_epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Create new unc model\n",
    "    new_unc_model = init_scod(model, dataloaders[\"all_train\"], dataset_name) \n",
    "\n",
    "    for phase in phases:\n",
    "        if phase[0] == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # batch loop\n",
    "        for d in dataloaders[phase[1]]:\n",
    "            if len(d) == 2:\n",
    "                inputs, labels = d \n",
    "            elif len(d) == 3:\n",
    "                inputs, labels, fnames = d \n",
    "            else:\n",
    "                raise ValueError(\"Unrecognized. Dataloader entry has length {}\".format(len(d)))\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase[0] == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                \n",
    "\n",
    "                if phase[0] == 'train':\n",
    "                    for i in inputs[:2]:\n",
    "                        unc = eval_scod(i, unc_model, dist_layer)\n",
    "                        unc = unc.item()\n",
    "                        print(\"Uncertainty: \", unc)\n",
    "                        LtJ = get_Lt_J(i.unsqueeze(0), unc_model, debug=False, dist_layer=dist_layer)\n",
    "                        orig_trace = np.trace(LtJ)\n",
    "                        print(\"LtJ shape is \", np.shape(LtJ))\n",
    "\n",
    "                        new_unc = eval_scod(i, new_unc_model, dist_layer)\n",
    "                        new_unc = new_unc.item()\n",
    "                        print(\"New Uncertainty: \", new_unc)\n",
    "                        new_LtJ = get_Lt_J(i.unsqueeze(0), new_unc_model, debug=False, dist_layer=dist_layer)\n",
    "                        diff = np.trace(new_LtJ - LtJ)\n",
    "                        print(\"Trace is \", diff/orig_trace)\n",
    "\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_sizes[phase[1]]\n",
    "        print('{} {} Loss: {:.4f}'.format(phase[0], phase[1], epoch_loss))\n",
    "        if phase[0] == 'train':\n",
    "            scheduler.step()\n",
    "        losses[phase[1]][epoch-start_epochs] = epoch_loss\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
